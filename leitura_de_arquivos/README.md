## Leitura de Dados (arquivos) no PySpark

Nesta se√ß√£o, organizei tudo que aprendi at√© agora sobre como **ler dados no PySpark**, incluindo os principais m√©todos, op√ß√µes de configura√ß√£o, erros comuns e boas pr√°ticas ‚Äî tudo com exemplos reais no ambiente Databricks.

---

## √çndice

- [O que √© DBFS?](https://github.com/AninhaBe/pyspark_zero_to_pro/blob/main/leitura_de_arquivos/README.md#o-que-%C3%A9-dbfs)  
- [Explorando os arquivos com DBFS](https://github.com/AninhaBe/pyspark_zero_to_pro/blob/main/leitura_de_arquivos/README.md#explorando-os-arquivos-com-dbfs)  
- [Lendo dados CSV](#-lendo-dados-csv)  
- [Erros comuns na leitura](#-erros-comuns-na-leitura)  
- [Lendo dados JSON](#-lendo-dados-json)  
- [Curiosidades sobre `.csv()` e `.json()`](#-curiosidades-sobre-csv-e-json)  
- [Resumo](#-resumo)

---

### O que √© DBFS?

- DBFS = **Databricks File System**
- Um sistema de arquivos virtual que roda sobre um storage na nuvem
- Voc√™ acessa com caminhos como:
  ```python
  'dbfs:/FileStore/arquivo.csv'
  ```

Pense nele como o ‚Äúgerenciador de arquivos‚Äù dentro do seu ambiente Databricks.

---

### Explorando os arquivos com DBFS

Antes de iniciar a leitura de arquivos com Spark, √© importante saber onde eles est√£o localizados. O Databricks oferece o **DBFS (Databricks File System)**, uma camada de abstra√ß√£o sobre o armazenamento em nuvem.

```python
dbutils.fs.ls("/FileStore/")
```

Esse comando √© **espec√≠fico do ambiente Databricks** e **n√£o funciona fora dele** (como em notebooks locais ou ambientes Python puros).

#### O que ele faz?

Ele **lista os arquivos e pastas** que est√£o armazenados no DBFS ‚Äî √© o equivalente ao `ls` no terminal Linux ou ao que fazemos no Windows Explorer ao abrir uma pasta.

#### Como ele √© estruturado?

```python
dbutils     # m√≥dulo interno do Databricks
   .fs      # funcionalidade espec√≠fica para interagir com o File System (DBFS)
      .ls() # fun√ß√£o que lista os arquivos e pastas dentro de um caminho
```

> **Ou seja**: estamos dizendo ao Databricks ‚Äúuse o utilit√°rio de sistema de arquivos e me diga o que tem nessa pasta‚Äù.

#### Para que ele serve na pr√°tica?

Usei o `dbutils.fs.ls()` principalmente para:
- Verificar se o arquivo que quero carregar com o Spark est√° realmente no DBFS
- Saber o **caminho completo** do arquivo (o `.path`) que vou passar para `.load()`
- Validar se o upload do arquivo foi feito com sucesso

---

### Lendo dados CSV

```python
df = (
    spark.read.format('csv')
    .option('inferSchema', True)
    .option('header', True)
    .load('dbfs:/FileStore/BigMart_Sales__1_.csv')
)
df.display()
```

Detalhes:
- `'inferSchema': True` ‚Üí o Spark tenta deduzir o tipo de cada coluna.
- `'header': True` ‚Üí usa a primeira linha como nome das colunas.
- `display()` ‚Üí fun√ß√£o do Databricks para visualiza√ß√£o r√°pida dos dados.

---

### Erros comuns na leitura

Durante meus testes, enfrentei um erro de indenta√ß√£o causado por **quebrar linhas com barra invertida (`\`) e adicionar coment√°rios na mesma linha**:

```python
.option('multiLine', False)\  # ‚ùå Isso causa erro
```

Solu√ß√£o recomendada: use **par√™nteses** ao redor de toda a express√£o encadeada:

```python
df = (
    spark.read.format('csv')
    .option('inferSchema', True)
    .option('header', True)
    .load('dbfs:/FileStore/arquivo.csv')
)
```

---

### Lendo dados JSON

```python
df_json = (
    spark.read.format('json')
    .option('inferSchema', True)
    .option('header', True)
    .option('multiLine', False)
    .load('dbfs:/FileStore/drivers.json')
)
df_json.display()
```

Use `'multiLine': True` se o JSON estiver formatado em v√°rias linhas.

---

### Curiosidades sobre `.csv()` e `.json()`

- `.csv()` e `.json()` s√£o **atalhos para `.format().load()`**
- Spark aceita dezenas de par√¢metros como:
  - `sep`, `encoding`, `nullValue`, `timestampFormat`, etc.
- Voc√™ pode ver tudo com:

```python
help(spark.read.csv)
```

üìö [Documenta√ß√£o CSV](https://spark.apache.org/docs/latest/sql-data-sources-csv.html)  
üìö [Documenta√ß√£o JSON](https://spark.apache.org/docs/latest/sql-data-sources-json.html)

---


### ‚úÖ Resumo

| A√ß√£o                        | Comando                             |
|-----------------------------|-------------------------------------|
| Listar arquivos no DBFS     | `dbutils.fs.ls("/FileStore/")`      |
| Ler CSV                     | `.read.format('csv').load(...)`     |
| Ler JSON                    | `.read.format('json').load(...)`    |
| Evitar erro de indenta√ß√£o   | Use par√™nteses ao quebrar linhas    |
| Ver op√ß√µes dispon√≠veis      | `help(spark.read.csv)`              |

---

### PS: O notebook desenvolvido e os arquivos que foram usados no databricks pode ser encontrado dentro dessa pasta.
