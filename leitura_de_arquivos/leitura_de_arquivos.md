
---

## ğŸ“¥ Leitura de Dados no PySpark

Nesta seÃ§Ã£o, organizei tudo que aprendi atÃ© agora sobre como **ler dados no PySpark**, incluindo os principais mÃ©todos, opÃ§Ãµes de configuraÃ§Ã£o, erros comuns e boas prÃ¡ticas â€” tudo com exemplos reais no ambiente Databricks.

---

### ğŸ“ Explorando os arquivos com DBFS

Antes de iniciar a leitura de arquivos com Spark, Ã© importante saber onde eles estÃ£o localizados. O Databricks oferece o **DBFS (Databricks File System)**, uma camada de abstraÃ§Ã£o sobre o armazenamento em nuvem.

Para listar arquivos e diretÃ³rios dentro do DBFS:

```python
dbutils.fs.ls("/FileStore/")
```

Isso funciona como um â€œWindows Explorerâ€ em nuvem. VocÃª pode verificar o nome dos arquivos e caminhos antes de carregÃ¡-los.

---

### ğŸ“„ Lendo dados CSV

```python
df = (
    spark.read.format('csv')
    .option('inferSchema', True)  # Infere automaticamente os tipos das colunas
    .option('header', True)       # Considera a primeira linha como cabeÃ§alho
    .load('dbfs:/FileStore/BigMart_Sales__1_.csv')
)
df.display()
```

ğŸ“Œ Detalhes importantes:
- `'inferSchema': True` â†’ o Spark analisa os dados e tenta entender automaticamente o tipo de cada coluna.
- `'header': True` â†’ usa a primeira linha como nome das colunas (em vez de tratÃ¡-la como dado).
- O mÃ©todo `display()` Ã© exclusivo do Databricks para exibir o DataFrame de forma visual.

---

### ğŸ§ª Erros comuns na leitura

Durante meus testes, enfrentei um erro de indentaÃ§Ã£o causado por **quebrar linhas com barra invertida (`\`) e adicionar comentÃ¡rios na mesma linha**:

```python
.option('multiLine', False)\  # âŒ Isso causa erro
```

âœ… SoluÃ§Ã£o recomendada: use **parÃªnteses** ao redor de toda a expressÃ£o encadeada:

```python
df = (
    spark.read.format('csv')
    .option('inferSchema', True)
    .option('header', True)
    .load('dbfs:/FileStore/arquivo.csv')
)
```

---

### ğŸ§¾ Lendo dados JSON

```python
df_json = (
    spark.read.format('json')
    .option('inferSchema', True)
    .option('header', True)
    .option('multiLine', False)  # Nesse arquivo JSON, os dados estÃ£o em uma linha sÃ³
    .load('dbfs:/FileStore/drivers.json')
)
df_json.display()
```

ğŸ“Œ Explicando a opÃ§Ã£o:
- `'multiLine': False` â†’ usado quando o JSON inteiro estÃ¡ em uma Ãºnica linha. Se o JSON for formatado com quebras de linha, usamos `True`.

---

### ğŸ§  Curiosidades sobre `.csv()` e `.json()`

- Ambos os mÃ©todos (`.csv()` e `.json()`) sÃ£o **atalhos** para `format().load()`, mas permitem configuraÃ§Ãµes especÃ­ficas.
- O Spark aceita **vÃ¡rios parÃ¢metros adicionais**, como `sep`, `encoding`, `nullValue`, `timestampFormat`, entre outros.
- VocÃª pode verificar todos os parÃ¢metros disponÃ­veis com:

```python
help(spark.read.csv)
```

Ou acessar a documentaÃ§Ã£o:
ğŸ”— https://spark.apache.org/docs/latest/sql-data-sources-csv.html  
ğŸ”— https://spark.apache.org/docs/latest/sql-data-sources-json.html

---

### ğŸ§  O que Ã© DBFS?

- **DBFS** = *Databricks File System*
- Ã‰ o â€œdisco virtualâ€ do Databricks, onde seus arquivos ficam armazenados
- Pode ser acessado com caminhos como:
  ```python
  'dbfs:/FileStore/arquivo.csv'
  ```

Ã‰ como o **gerenciador de arquivos do Databricks**, e pode ser acessado via cÃ³digo ou interface grÃ¡fica.

---

### âœ… Resumo

| AÃ§Ã£o                        | Comando                             |
|-----------------------------|-------------------------------------|
| Listar arquivos no DBFS     | `dbutils.fs.ls("/FileStore/")`      |
| Ler CSV                     | `.read.format('csv').load(...)`     |
| Ler JSON                    | `.read.format('json').load(...)`    |
| Evitar erro de indentaÃ§Ã£o   | Use parÃªnteses ao quebrar linhas    |
| Ver opÃ§Ãµes disponÃ­veis      | `help(spark.read.csv)`              |

---
