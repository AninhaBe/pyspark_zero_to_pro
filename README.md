# **PySpark Zero to Pro**  
**Documentando meu aprendizado sobre Apache Spark e PySpark**  

Este reposit√≥rio cont√©m meus estudos e experimentos sobre **Apache Spark**, explicando sua **arquitetura, funcionamento e como ele processa grandes volumes de dados**. Aqui, estou registrando minha compreens√£o do assunto, aprimorando conceitos ao longo do caminho.  

---

## **√çndice**
- [üìå O que √© Apache Spark?](#-o-que-√©-apache-spark)
- [‚öôÔ∏è Arquitetura do Spark](https://github.com/AninhaBe/pyspark_zero_to_pro/blob/main/README.md#%EF%B8%8F-arquitetura-do-spark)
- [üöÄ Fluxo de Execu√ß√£o do Spark](#-fluxo-de-execu√ß√£o-do-spark)
- [üìö Recursos para Estudo](#-recursos-para-estudo)
- [üìå Como Rodar este Reposit√≥rio](#-como-rodar-este-reposit√≥rio)

---

## **O que √© Apache Spark?**
Apache Spark √© um **framework open-source** que facilita o **processamento de dados em larga escala**.  

Ou seja, o Spark √© um mecanismo de **computa√ß√£o distribu√≠da** que permite **dividir e distribuir dados entre v√°rias m√°quinas** para process√°-los de forma **mais eficiente**.  

Imagine que voc√™ tenha **1GB de dados** para processar. Voc√™ pode:
1. **Rodar em uma √∫nica m√°quina** e tentar aumentar seus recursos (CPU, mem√≥ria).  
2. **Dividir o trabalho entre v√°rias m√°quinas** e conect√°-las, formando um **cluster**.  

No segundo caso, cada m√°quina do cluster processa **uma parte dos dados**, tornando o processo **mais r√°pido e eficiente**.  

**Exemplo do mundo real**:  
Se voc√™ est√° na faculdade e tem **milhares de computadores dispon√≠veis**, o Spark permite que voc√™ use **todas essas m√°quinas juntas**, aproveitando o poder de processamento delas.  

### **Por que usar Spark?**
‚úÖ **Processamento Distribu√≠do** ‚Üí Permite trabalhar com **Big Data**.  
‚úÖ **Alta Performance** ‚Üí Pode ser **100x mais r√°pido** que outros frameworks.  
‚úÖ **Processamento em Mem√≥ria (In-Memory)** ‚Üí Evita leituras e grava√ß√µes no disco.  
‚úÖ **Funciona com SQL, Machine Learning e Streaming de dados**.  

Se um c√≥digo Python tradicional demoraria **horas** para processar um grande conjunto de dados, o Spark pode fazer isso **em minutos** usando m√∫ltiplas m√°quinas.  

---

## **Arquitetura do Spark**
A arquitetura do Spark √© **baseada em clusters** e segue um modelo **Mestre-Escravo (Master-Slave)**.  

A imagem abaixo representa essa estrutura:  
![cluster-overview](https://github.com/user-attachments/assets/34ea3450-581b-4c2d-9c51-dcbb9fac7758)


### **1. Driver Program (Programa Principal)**
O **Driver Program** √© onde tudo come√ßa!  
- Ele **inicializa a sess√£o Spark**, criando o **SparkContext**, que ser√° respons√°vel por gerenciar os n√≥s.  
- Ele **divide as tarefas** e **coordena o processamento** dentro do cluster.  

**Exemplo no PySpark**:
```python
from pyspark.sql import SparkSession

spark = SparkSession.builder.appName("MeuApp").getOrCreate()
```
**Aqui, o `SparkSession` cria o Driver Program.**

---

### **2. Cluster Manager (Gerenciador de Recursos)**
- O **Cluster Manager** √© respons√°vel por gerenciar os recursos do cluster.  
- Ele decide **quantos n√≥s ser√£o usados**, **quantos executores ser√£o criados** e **quantos recursos cada executor pode ter**.  
- Ele recebe as **requisi√ß√µes do SparkContext** e distribui as tarefas.  

O Spark pode rodar com diferentes **gerenciadores de cluster**:
- **Standalone** (modo b√°sico do pr√≥prio Spark).  
- **YARN** (Hadoop).  
- **Mesos**.  
- **Kubernetes** (gerenciador na nuvem).  

**O Cluster Manager distribui os recursos de acordo com a demanda do SparkContext!**

---

### **3. Worker Nodes (N√≥s Trabalhadores)**
Os **Worker Nodes** s√£o as m√°quinas onde **o processamento realmente acontece**.  
Cada Worker cont√©m:  
‚úî **Executors** ‚Üí Executam as tarefas que foram enviadas pelo Driver.  
‚úî **Tasks** ‚Üí Pequenas partes do c√≥digo que ser√£o rodadas em paralelo.  
‚úî **Cache** ‚Üí Armazena dados em mem√≥ria para melhorar a performance.  

**Quanto mais Worker Nodes dispon√≠veis, mais r√°pido ser√° o processamento!** üöÄ

---

## **Fluxo de Execu√ß√£o do Spark**
Agora que entendemos os componentes, veja **como o Spark processa os dados** passo a passo:

1Ô∏è‚É£ O **Driver Program** inicia a sess√£o Spark e cria um **SparkContext**.  
2Ô∏è‚É£ O **SparkContext** conversa com o **Cluster Manager** e solicita recursos.  
3Ô∏è‚É£ O **Cluster Manager** distribui os recursos e inicia os **Executors** nos **Worker Nodes**.  
4Ô∏è‚É£ O **Driver Program** envia **tarefas (Tasks)** para os **Executors** processarem os dados.  
5Ô∏è‚É£ Cada **Worker Node** processa suas tarefas **em paralelo** e pode armazenar dados em **cache**.  
6Ô∏è‚É£ Os **Executors** retornam os resultados ao **Driver**, que une tudo e gera a sa√≠da final.  

---

## **Recursos para Estudo**
üîπ [Documenta√ß√£o Oficial do Spark](https://spark.apache.org/docs/latest/)  
üîπ [Curso Gratuito de PySpark no Databricks](https://www.databricks.com/learn)  
üîπ [Livro: Learning Spark - O'Reilly](https://www.oreilly.com/library/view/learning-spark-2nd/9781492050032/)  

---

## **Como Rodar este Reposit√≥rio**
Para rodar os exemplos de PySpark localmente, siga estes passos:

1Ô∏è‚É£ **Instale o PySpark**  
```sh
pip install pyspark
```

2Ô∏è‚É£ **Crie uma SparkSession no Python**
```python
from pyspark.sql import SparkSession

spark = SparkSession.builder.appName("MeuApp").getOrCreate()
```

3Ô∏è‚É£ **Rode um exemplo simples**
```python
data = [(1, "Alice"), (2, "Bob"), (3, "Charlie")]
df = spark.createDataFrame(data, ["id", "nome"])
df.show()
```


